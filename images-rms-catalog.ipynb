{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve current AWS account and Region"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this cell, we are using boto3 library to get the current AWS account and AWS region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "current_region = sts_client.meta.region_name\n",
    "\n",
    "print(f\"Current AWS Account ID: {account_id}\")\n",
    "print(f\"Current region : {current_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T17:37:03.385236Z",
     "iopub.status.busy": "2025-01-07T17:37:03.384649Z",
     "iopub.status.idle": "2025-01-07T17:37:03.389036Z",
     "shell.execute_reply": "2025-01-07T17:37:03.387994Z",
     "shell.execute_reply.started": "2025-01-07T17:37:03.385209Z"
    }
   },
   "source": [
    "## Configure our Spark Session to use a RMS backed catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes a PySpark session with specific configurations for working with Apache Iceberg tables through AWS Glue catalog. It sets up:\n",
    "\n",
    "1. A custom catalog named 'images'\n",
    "2. AWS Glue as the catalog implementation\n",
    "3. Iceberg extensions for Spark\n",
    "The session is configured to use Iceberg as the table format and AWS Glue as the metadata store, enabling data lake operations\n",
    "\n",
    "In the code below, replace the Account-number with the AWS Account Id you get from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark emr-s.emr-image-processing\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "\n",
    "spark = SparkSession.builder.appName('rms_images') \\\n",
    ".config('spark.sql.catalog.images', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.defaultCatalog', 'images') \\\n",
    ".config('spark.sql.catalog.images.catalog-impl', 'org.apache.iceberg.aws.glue.GlueCatalog') \\\n",
    ".config('spark.sql.catalog.images.glue.id','Account-number:image-data-rms/dev') \\\n",
    ".config('spark.sql.catalog.images.client.region','us-east-1') \\\n",
    ".config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's configure the spark session to use the public namespace/database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark emr-s.emr-image-processing\n",
    "spark.sql(\"use public\")\n",
    "\n",
    "spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframe from the JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a data processing pipeline using PySpark. It defines a structured schema for the images dataset. The code then reads a JSON file from an S3 location using this schema, and loads it into a DataFrame.  The code then drops one of the columns  and creates a temporary SQL view  for potential SQL queries. \n",
    "                                                                                                                                                                    \n",
    "In the code section below, replace *insert your S3 URI here* with the  S3 URI you saved after uploading the images JSON file.                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark emr-s.emr-image-processing\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Define the schema for the nested struct in associated_entities\n",
    "associated_entities_schema = StructType([\n",
    "    StructField(\"entity_submitter_id\", StringType(), True),\n",
    "    StructField(\"entity_type\", StringType(), True),\n",
    "    StructField(\"case_id\", StringType(), True),\n",
    "    StructField(\"entity_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the main schema\n",
    "images_schema = StructType([\n",
    "    StructField(\"data_format\", StringType(), True),\n",
    "    StructField(\"access\", StringType(), True),\n",
    "    StructField(\"associated_entities\", ArrayType(associated_entities_schema), True),\n",
    "    StructField(\"s3_uri\", StringType(), True),\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"md5sum\", StringType(), True),\n",
    "    StructField(\"file_id\", StringType(), True),\n",
    "    StructField(\"data_type\", StringType(), True),\n",
    "    StructField(\"submitter_id\", StringType(), True),\n",
    "    StructField(\"data_category\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"experimental_strategy\", StringType(), True),\n",
    "    StructField(\"file_size\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Now you can use this schema to create a DataFrame\n",
    "# For example:\n",
    "#df = spark.createDataFrame(data, schema=images_schema)\n",
    "\n",
    "# Or when reading from a data source:\n",
    "df = spark.read.schema(images_schema).json(\"insert your S3 URI here\")\n",
    "\n",
    "\n",
    "# Show the first few rows\n",
    "df.show(5)\n",
    "\n",
    "# Print the schema to verify\n",
    "df.printSchema()\n",
    "\n",
    "# Get basic statistics of the numerical columns\n",
    "#df.describe().show()\n",
    "\n",
    "# Count total rows\n",
    "print(f\"Total number of records: {df.count()}\")\n",
    "\n",
    "df_dropped = df.drop(\"associated_entities\")\n",
    "\n",
    "df_dropped.show(5)\n",
    "\n",
    "df_dropped.createOrReplaceTempView(\"view_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T12:09:53.936749Z",
     "iopub.status.busy": "2024-12-19T12:09:53.932284Z"
    }
   },
   "source": [
    "## Create Iceberg table in RMS catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a new table named 'images_rms_table' if it doesn't already exist. The table uses the Apache Iceberg format and is configured with RMS as the AWS write format through table properties.\n",
    "\n",
    "If the SQL fails the first time, re-run the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark emr-s.emr-image-processing\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS images_rms_table (\n",
    "    data_format STRING,\n",
    "    access STRING,\n",
    "    s3_uri STRING ,\n",
    "    file_name STRING ,\n",
    "    md5sum STRING ,\n",
    "    file_id STRING ,\n",
    "    data_type STRING ,\n",
    "    submitter_id STRING ,\n",
    "    data_category STRING ,\n",
    "    state STRING ,\n",
    "    experimental_strategy STRING ,\n",
    "    file_size INTEGER \n",
    ")USING iceberg TBLPROPERTIES ('aws.write.format'='RMS')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data into iceberg table from temporary view"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This SQL command inserts all records from the view named \"view_images\" into a table called \"images_rms_table\" using Spark SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark emr-s.emr-image-processing\n",
    "spark.sql(\"insert into public.images_rms_table select * from view_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark emr-s.emr-image-processing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
